{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation and numerical operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Mutual information and feature selection\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# Statistical computations (optional for advanced use cases)\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Handling imbalanced data\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# Model explainability\n",
    "from alibi.explainers.ale import ALE, plot_ale\n",
    "from sklearn.inspection import partial_dependence, PartialDependenceDisplay\n",
    "\n",
    "# Permutation Importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# SHAP Values\n",
    "import shap\n",
    "\n",
    "# Suppress FutureWarnings\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the Data\n",
    "\n",
    "folder_path = \"./data/\"\n",
    "#train_identity = pd.read_csv(f\"{folder_path}train_identity.csv\")\n",
    "#train_transaction = pd.read_csv(f\"{folder_path}train_transaction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine the datasets\n",
    "#train = pd.merge(train_transaction, train_identity, on=\"TransactionID\", how=\"left\")\n",
    "\n",
    "## Replace inf and -inf values with NaN in the dataset\n",
    "#train.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f\"{folder_path}train_selected_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (472432, 15)\n",
      "Testing data shape: (118108, 15)\n"
     ]
    }
   ],
   "source": [
    "X = df_train.drop(columns=['isFraud'])  # Features (all columns except 'isFraud')\n",
    "y = df_train['isFraud']   \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_train)\n",
    "x_train_scaled = model.transform(X_train)\n",
    "\n",
    "x_test_scaled = model.transform(X_test)\n",
    "\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95    113866\n",
      "           1       0.28      0.84      0.42      4242\n",
      "\n",
      "    accuracy                           0.92    118108\n",
      "   macro avg       0.63      0.88      0.69    118108\n",
      "weighted avg       0.97      0.92      0.94    118108\n",
      "\n",
      "Confusion Matrix:\n",
      " [[104554   9312]\n",
      " [   684   3558]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ALEJANDRO\\Documents\\5. Programming\\.virtualenvs\\aplt_duke\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but BalancedRandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.6223372776703963\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Train a Balanced Random Forest model\n",
    "brf = BalancedRandomForestClassifier(n_estimators=100, random_state=42)\n",
    "brf.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_pred = brf.predict(x_test_scaled)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, brf.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ALEJANDRO\\Documents\\5. Programming\\.virtualenvs\\aplt_duke\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\ALEJANDRO\\Documents\\5. Programming\\.virtualenvs\\aplt_duke\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\ALEJANDRO\\Documents\\5. Programming\\.virtualenvs\\aplt_duke\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9662469940969051\n",
      "SVC 0.9675805195062906\n",
      "LinearSVC 0.9656924173155567\n",
      "KNeighbors 0.9668735394093244\n",
      "DecisionTree 0.9677731403889194\n",
      "RandomForest 0.9818090219459886\n",
      "RandomForest2 0.9746566699106763\n",
      "MLPClassifier 0.9716805801404096\n",
      "GaussianNB 0.9567620305811434\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "models.append((\"LogisticRegression\",LogisticRegression()))\n",
    "models.append((\"SVC\",SVC()))\n",
    "models.append((\"LinearSVC\",LinearSVC()))\n",
    "models.append((\"KNeighbors\",KNeighborsClassifier()))\n",
    "models.append((\"DecisionTree\",DecisionTreeClassifier()))\n",
    "models.append((\"RandomForest\",RandomForestClassifier()))\n",
    "rf2 = RandomForestClassifier(n_estimators=100, criterion='gini',\n",
    "                                max_depth=10, random_state=0, max_features=None)\n",
    "models.append((\"RandomForest2\",rf2))\n",
    "models.append((\"MLPClassifier\",MLPClassifier(solver='lbfgs', random_state=0)))\n",
    "models.append((\"GaussianNB\",GaussianNB()))\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name,model in models:\n",
    "    result = cross_val_score(model, x_train_scaled,y_train,  cv=3)\n",
    "    names.append(name)\n",
    "    results.append(result)\n",
    "\n",
    "for i in range(len(names)):\n",
    "    print(names[i],results[i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the test set and sample a smaller subset\n",
    "X_test_copy = X_test.copy()\n",
    "X_test_sample = X_test_copy.sample(10_000, random_state=42)  # Sample 2000 rows\n",
    "\n",
    "# Initialize ALE explainer with all features\n",
    "ale = ALE(lambda x: brf.predict_proba(x)[:, 1], feature_names=X_test.columns)\n",
    "\n",
    "# Compute ALE explanation on the sampled dataset\n",
    "ale_explanation = ale.explain(X_test_sample.values)\n",
    "\n",
    "# Plot ALE for the top 15 features\n",
    "for i, feature in enumerate(X_test.columns):\n",
    "    feature_idx = [list(X_test.columns).index(feature)]  # Make feature_idx a list\n",
    "    plot_ale(ale_explanation, features=feature_idx)  # Use plot_ale for visualization\n",
    "    plt.title(f\"ALE Plot for {feature}\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Effect on Prediction\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aplt_duke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
